近年来，以卷积神经网络（Convolutional Neural Network, CNN）为代表的深度学习技术在遥感影像语义分割任务中得到广泛应用。基于全卷积网络（Fully Convolutional Network, FCN）[1]结构的发展，使得端到端的像素级分割成为可能。此后，U-Net[2]通过编码器-解码器结构与跳跃连接机制实现了多尺度特征融合，DeepLabv3+[3]引入空洞空间金字塔池化模块增强了多尺度感知能力。然而，高分辨率遥感影像具有显著的尺度变化和复杂的空间异质性，不同地物在空间尺度上差异明显，且类间相似性与类内差异性并存，导致模型易出现边界模糊和类别混淆问题。传统CNN方法受限于局部感受野，难以在复杂遥感场景中同时兼顾全局语义一致性与局部细节精度。
为弥补CNN在全局信息建模方面的局限，研究者开始将Transformer结构引入遥感图像语义分割任务。Segmenter[4]采用纯Transformer架构实现了全局上下文建模，SegFormer[5]通过层次化编码器设计兼顾了多尺度特征提取与计算效率，BANet[6]利用双边注意力机制增强了边界感知能力，UNetFormer[7]则将Transformer与U-Net结构相结合，在遥感分割任务中取得了优异性能。然而，此类方法在高分辨率遥感影像场景中普遍面临计算开销较大的问题，其时间复杂度与空间复杂度均随分辨率呈二次增长，限制了其在大尺度遥感场景中的实际应用。此外，大多数基于Transformer的模型仍依赖单一特征通路进行语义建模，其对局部结构信息的刻画能力有限，在处理细粒度边界和复杂结构地物时仍存在不足。
近年来，状态空间模型（State Space Model, SSM）及其视觉扩展形式在序列建模和长程依赖捕获方面表现出较强优势，并逐步被引入视觉任务中。在遥感领域，RS-Mamba[13]和Samba[14]等方法尝试将状态空间模型引入遥感影像分析任务，取得了初步成效。这些方法通常将状态空间模块与CNN结构进行组合，利用状态空间模型的线性复杂度优势实现高效的全局建模。然而，现有工作多停留在注意力模块的替换或堆叠层面，未能充分挖掘两类模型的协同潜力。具体而言，现有方法普遍存在以下两方面不足：一是架构耦合问题，即全局上下文建模与局部细节提取在单一路径中堆叠处理，两类本质不同的任务共享相同的参数空间与计算流程，导致模型难以对任一任务实现最优建模；二是注意力冗余问题，即在网络中重复应用相似的注意力机制，虽在一定程度上增强了特征聚焦能力，但也带来了不必要的计算开销与参数冗余，影响了模型的整体效率。
