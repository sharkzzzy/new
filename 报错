近年来，以卷积神经网络（Convolutional Neural Network, CNN）为代表的深度学习技术在计算机视觉领域取得了突破性进展，并逐步被引入遥感影像语义分割任务中。基于全卷积网络（Fully Convolutional Network, FCN）[1]结构的发展，使得端到端的像素级分割成为可能。此后，U-Net[2]通过编码器-解码器结构与跳跃连接机制实现了多尺度特征融合，DeepLabv3+[3]则引入空洞空间金字塔池化模块增强了多尺度感知能力，这些方法在遥感影像分割任务中得到了广泛应用。然而，高分辨率遥感影像在成像机理和目标分布方面的特殊性，使其呈现出显著的尺度变化和复杂的空间异质性：一方面，不同地物在空间尺度上差异明显，大尺度区域目标与小尺度目标在同一幅影像中共存，增加了特征建模难度；另一方面，不同类别之间在光谱表现上的相似性与类内差异性并存，导致模型易出现边界模糊和类别混淆问题。传统基于CNN的方法受限于固定大小的局部感受野，在建模长距离空间依赖关系时存在明显不足，难以在复杂遥感场景中同时兼顾全局语义一致性与局部细节精度。

为弥补CNN在全局信息建模方面的局限，近年来研究者开始将自注意力机制与Transformer结构引入遥感图像语义分割任务。ViT[4]首次将纯Transformer架构应用于图像分类任务，证明了自注意力机制在视觉任务中的有效性。此后，Swin Transformer[5]通过滑动窗口机制降低了计算复杂度，Segmenter[6]和SegFormer[7]等方法进一步将Transformer架构扩展至语义分割任务。在遥感领域，BANet[8]和UNetFormer[9]等方法通过结合Transformer与CNN的优势，在多个遥感分割基准上取得了优异性能。相关研究表明，基于Transformer的模型能够通过全局建模机制有效捕获长程依赖关系，从而提升复杂场景下的语义一致性。然而，此类方法在高分辨率遥感影像场景中普遍面临计算开销较大的问题，其时间复杂度与空间复杂度均随分辨率呈二次增长，限制了其在大尺度遥感场景中的实际应用。此外，大多数基于Transformer的模型仍依赖单一特征通路进行语义建模，其对局部结构信息的刻画能力有限，在处理细粒度边界和复杂结构地物时仍存在不足。

近年来，状态空间模型（State Space Model, SSM）及其视觉扩展形式在序列建模和长程依赖捕获方面表现出较强优势，并逐步被引入视觉任务中。Mamba[10]通过选择性状态空间机制实现了高效的长序列建模，VMamba[11]和Vim[12]等工作将其扩展至视觉领域，在图像分类和分割任务中展现出良好的性能与效率平衡。在遥感领域，RS-Mamba[13]和Samba[14]等方法尝试将状态空间模型引入遥感影像分析任务，取得了初步成效。这些方法通常将状态空间模块与CNN结构进行组合，利用状态空间模型的线性复杂度优势实现高效的全局建模。然而，现有工作多停留在注意力模块的替换或堆叠层面，未能充分挖掘两类模型的协同潜力。具体而言，现有方法普遍存在以下两方面不足：一是架构耦合问题，即全局上下文建模与局部细节提取在单一路径中堆叠处理，两类本质不同的任务共享相同的参数空间与计算流程，导致模型难以对任一任务实现最优建模；二是注意力冗余问题，即在网络中重复应用相似的注意力机制，虽在一定程度上增强了特征聚焦能力，但也带来了不必要的计算开销与参数冗余，影响了模型的整体效率。
