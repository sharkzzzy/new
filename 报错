近年来，以卷积神经网络（Convolutional Neural Network, CNN）为代表的深度学习技术在计算机视觉领域取得了突破性进展，并逐步被引入遥感影像语义分割任务中。基于全卷积网络（Fully Convolutional Network, FCN）结构的发展，使得端到端的像素级分割成为可能。然而，高分辨率遥感影像在成像机理和目标分布方面的特殊性，使其呈现出显著的尺度变化和复杂的空间异质性：一方面，不同地物在空间尺度上差异明显，大尺度区域目标与小尺度目标在同一幅影像中共存，增加了特征建模难度；另一方面，不同类别之间在光谱表现上的相似性与类内差异性并存，导致模型易出现边界模糊和类别混淆问题。传统基于 CNN 的方法受限于固定大小的局部感受野，在建模长距离空间依赖关系时存在明显不足，难以在复杂遥感场景中同时兼顾全局语义一致性与局部细节精度。
为弥补 CNN 在全局信息建模方面的局限，近年来研究者开始将自注意力机制与 Transformer 结构引入遥感图像语义分割任务。相关研究表明，基于 Transformer 的模型能够通过全局建模机制有效捕获长程依赖关系，从而提升复杂场景下的语义一致性。然而，此类方法在高分辨率遥感影像场景中普遍面临计算开销较大的问题，其时间复杂度与空间复杂度均随分辨率呈二次增长，限制了其在大尺度遥感场景中的实际应用。此外，大多数 Transformer-based 模型仍依赖单一特征通路进行语义建模，其对局部结构信息的刻画能力有限，在处理细粒度边界和复杂结构地物时仍存在不足。
近年来状态空间模型（State Space Model, SSM）及其视觉扩展形式在序列建模和长程依赖捕获方面表现出较强优势，并逐步被引入视觉任务中。相关研究表明，基于状态空间模型的结构在保持线性计算复杂度的同时，具备良好的全局建模能力，为解决高分辨率图像场景下的效率瓶颈问题提供了新的技术路径。然而，现有将状态空间模型引入遥感语义分割的工作，多停留在注意力模块的替换或堆叠层面，未能充分挖掘状态空间模型与CNN的协同潜力。具体而言，现有方法普遍存在以下两方面不足：一是架构耦合问题，即全局上下文建模与局部细节提取在单一路径中堆叠处理，两类本质不同的任务共享相同的参数空间与计算流程，导致模型难以对任一任务实现最优建模；二是注意力冗余问题，即在网络中重复应用相似的注意力机制，虽在一定程度上增强了特征聚焦能力，但也带来了不必要的计算开销与参数冗余，影响了模型的整体效率。
